#!/usr/bin/env python
"""
Process Twitter stream files for food counts.
"""
from argparse import ArgumentParser
from noweats.extraction import filters_from_dict, read_json, \
    pos_tag_clean_text_data, chunk_parse, count_foods
from noweats.analysis import merge_most_common_counts, find_interesting
from multiprocessing import Pool

import os
import sys
import json

_EAT_LEXICON = ['eat', 'ate', 'eating']

_MERGE_TOP_K = 200

_NUM_INTERESTING = 50

def process_files(file_paths, output_dir, eat_lexicon, filters,
                  merge_top_k, num_interesting):
    """ Process data files. """

    def chunks(listdata, size):
        """
        Yield successive n-sized chunks from l.

        From http://stackoverflow.com/questions/312443/
        how-do-you-split-a-list-into-evenly-sized-chunks-in-python
        """
        for i in xrange(0, len(listdata), size):
            yield listdata[i:i + size]

    pool = Pool()

    for path in file_paths:

        data_json = read_json(path)

        # Process in parallel. These parts be slow.
        pos_tagged = [tagged for chunk in
                      pool.map(pos_tag_clean_text_data,
                               chunks(data_json, 1000))
                      for tagged in chunk]

        counts = count_foods(chunk_parse(pos_tagged), eat_lexicon, filters)

        merged_counts = merge_most_common_counts(counts, merge_top_k)
        interesting = find_interesting(counts, num_interesting)

        # Save counts and interesting to output directory.

        filename = os.path.basename(path)

        counts_path = os.path.join(output_dir,
                                   '{}.counts'.format(filename))
        with open(counts_path, 'w') as filep:
            json.dump(merged_counts, filep)

        interesting_path = os.path.join(output_dir,
                                        '{}.interesting'.format(filename))
        with open(interesting_path, 'w') as filep:
            json.dump(interesting, filep)


def main():
    """ Process data files. """

    home_dir = os.path.expanduser('~')
    default_conf_dir = os.path.join(home_dir, '.noweats')

    parser = ArgumentParser(description=
                            "Process a file containing streamed "
                            "Tweets to extract food counts.")

    parser.add_argument('-c', '--conf-dir', help="path to configuration data",
                        type=str, default=default_conf_dir)

    parser.add_argument('-o', '--output-dir', help="path to output data dir",
                        type=str, required=True)

    parser.add_argument('file_paths', help="paths to input files",
                        nargs='+')

    args = parser.parse_args()

    if not os.path.isdir(args.output_dir):
        os.mkdir(args.output_dir)

    # Read filters from conf dir.
    filters_path = os.path.join(args.conf_dir, 'filters.conf')
    if not os.path.isfile(filters_path):
        print >> sys.stderr, "Warning: cannot find filters " \
            "in config dir {}".format(args.conf_dir)
        filters = []
    else:
        filters = filters_from_dict(json.load(open(filters_path, 'r')))

    # Process files.
    process_files(args.file_paths, args.output_dir,
                  _EAT_LEXICON, filters,
                  _MERGE_TOP_K, _NUM_INTERESTING)


if __name__ == '__main__':
    main()
