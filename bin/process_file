#!/usr/bin/env python
"""
Process Twitter stream files for food counts.
"""
from argparse import ArgumentParser
from noweats.extraction import filters_from_dict, read_json, \
    sentence_split_clean_data, tokenize_tweet, \
    pos_tag_tweet, chunk_tweet, count_foods, remove_dups
from noweats.analysis import merge_most_common_counts, find_interesting
from multiprocessing import Pool

import os
import json
import pickle
import logging

_EAT_LEXICON = ['eat', 'ate', 'eating']

_MERGE_TOP_K = 200

_NUM_INTERESTING = 50

logging.basicConfig(level=logging.INFO)
_LOGGER = logging.getLogger("process_file")


def process_files(file_paths, output_dir,
                  eat_lexicon, filters,
                  merge_top_k, num_interesting):
    """ Process data files. """

    pool = Pool()

    for path in file_paths:

        try:
            filename = os.path.basename(path)
            data_json = read_json(path)
            tweets = sentence_split_clean_data(data_json, eat_lexicon)
            tweets = remove_dups(tweets)
            tokenized = pool.map(tokenize_tweet, tweets, 2000)

            # Process in parallel. These parts be slow.
            pos_tagged = pool.map(pos_tag_tweet, tokenized, 1000)

            ## Save POS tagged data in case we wish to resume.
            #tagged_path = os.path.join(output_dir,
            #                        '{}.tagged'.format(filename))
            #with open(tagged_path, 'w') as filep:
            #    pickle.dump(pos_tagged, filep)

            chunked = [chunk_tweet(tweet) for tweet in pos_tagged]
            counts = count_foods(chunked, eat_lexicon, filters)

            merged_counts = merge_most_common_counts(counts, merge_top_k)
            interesting = find_interesting(counts, num_interesting)

            # Save counts and interesting to output directory.
            counts_path = os.path.join(output_dir,
                                    '{}.counts'.format(filename))
            with open(counts_path, 'w') as filep:
                json.dump(merged_counts, filep)

            interesting_path = os.path.join(output_dir,
                                            '{}.interesting'.format(filename))
            with open(interesting_path, 'w') as filep:
                json.dump(interesting, filep)

        except Exception:
            _LOGGER.exception("Error processing file {}".format(filename))


def main():
    """ Process data files. """

    home_dir = os.path.expanduser('~')
    default_conf_dir = os.path.join(home_dir, '.noweats')

    parser = ArgumentParser(description=
                            "Process a file containing streamed "
                            "Tweets to extract food counts.")

    parser.add_argument('-c', '--conf-dir', help="path to configuration data",
                        type=str, default=default_conf_dir)

    parser.add_argument('-o', '--output-dir', help="path to output data dir",
                        type=str, required=True)

    parser.add_argument('file_paths', help="paths to input files",
                        nargs='+')

    args = parser.parse_args()

    if not os.path.isdir(args.output_dir):
        os.mkdir(args.output_dir)

    # Read filters from conf dir.
    filters_path = os.path.join(args.conf_dir, 'filters.conf')
    if not os.path.isfile(filters_path):
        _LOGGER.warning("Warning: cannot find filters " \
                        "in config dir {}".format(args.conf_dir))
        filters = []
    else:
        filters = filters_from_dict(json.load(open(filters_path, 'r')))

    # Process files.
    process_files(args.file_paths, args.output_dir,
                  _EAT_LEXICON, filters,
                  _MERGE_TOP_K, _NUM_INTERESTING)


if __name__ == '__main__':
    main()
